# Binary Embeddings Comparison

This project compares the performance of float embeddings versus binary embeddings for semantic search using Qdrant vector database and Ollama embeddings.

## Overview

The project evaluates the trade-offs between traditional float embeddings and binary embeddings by comparing:
- Search quality metrics (nDCG, MRR, Precision, Recall)
- Search speed performance
- Storage requirements

Binary embeddings are generated by converting float embeddings to binary representations (sign-based thresholding), which significantly reduces storage requirements and can improve search speed with minimal quality degradation.

## Project Structure

```
binary-embeddings/
├── src/
│   ├── dataset_prep.py      # Prepare dataset and create Qdrant collections
│   ├── query_quadrant.py    # Query Qdrant collections
│   └── evaluate.py          # Evaluate and compare float vs binary embeddings
├── data/                    # Data files and results
└── .venv/                   # Virtual environment
```

## Requirements

- Python 3.8+
- Qdrant server (running locally or remotely)
- Ollama server with nomic-embed-text model
- Dataset in JSONL format with `document_text` and `question_text` fields

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/binary-embeddings.git
cd binary-embeddings
```

2. Create and activate virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up required services:

**Qdrant:**
```bash
# Using Docker (recommended)
docker run -p 6333:6333 qdrant/qdrant

# Or download and run locally from https://qdrant.tech/documentation/quick-start/
```

**Ollama with nomic-embed-text:**
```bash
# Install Ollama from https://ollama.ai
# Pull the embedding model
ollama pull nomic-embed-text
```

5. Prepare your dataset:

   - Place your JSONL file in the `data/` directory
   - Each line should be a JSON object with `document_text` and `question_text` fields
   - Example format:

     ```json
     {"document_text": "Sample document content...", "question_text": "What is the sample about?"}
     ```

## Usage

### 1. Prepare Dataset

Generate embeddings and populate Qdrant collections with both float and binary embeddings:

```bash
python src/dataset_prep.py \
  --input-path data/simplified-nq-train.jsonl \
  --output-path data/test_dataset.jsonl \
  --max-entities 1000 \
  --qdrant-host localhost \
  --qdrant-port 6333 \
  --ollama-host http://rachel:11434 \
  --embedding-model nomic-embed-text
```

**Options:**
- `--input-path`: Path to input JSONL file with question/document pairs
- `--output-path`: Path to save test dataset
- `--max-entities`: Number of entities to process (-1 for unlimited)
- `--vector-size`: Embedding vector size (default: 768)
- `--float-collection`: Name for float embeddings collection
- `--binary-collection`: Name for binary embeddings collection

### 2. Query Collections

Search either float or binary embedding collections:

```bash
python src/query_quadrant.py "your query text here" \
  --collection nq_float_embeddings \
  -k 10 \
  --qdrant-host localhost \
  --qdrant-port 6333 \
  --ollama-host http://rachel:11434
```

**Options:**
- `--collection`: Collection to query (nq_float_embeddings or nq_binary_embeddings)
- `-k`: Number of results to return
- `--ids-only`: Return only result IDs

### 3. Evaluate Performance

Compare float vs binary embeddings across multiple metrics:

```bash
python src/evaluate.py \
  --num-questions 100 \
  --test-file data/test_dataset.jsonl \
  --output-dir data/results \
  --verbose
```

**Options:**
- `--num-questions`: Number of random questions to evaluate
- `--test-file`: Path to test dataset
- `--output-dir`: Directory to save results and plots
- `--verbose`: Generate comparison plots and visualizations
- `--seed`: Random seed for reproducibility

The evaluation generates:
- Summary statistics comparing float vs binary embeddings
- Metrics: nDCG@10, nDCG@5, MRR, Precision@k, Recall@k
- Search time comparison
- Storage size comparison
- Visualization plots (with `--verbose` flag)

## How It Works

### Binary Embedding Conversion

Float embeddings are converted to binary embeddings using sign-based thresholding:
1. Each float value is converted to 1 if positive, 0 otherwise
2. Binary values are packed into bytes (8 bits per byte)
3. A 768-dimensional float embedding becomes a 96-byte binary embedding

### Distance Metrics

- **Float embeddings**: Cosine similarity
- **Binary embeddings**: Manhattan distance (Hamming distance on packed bytes)

### Evaluation Metrics

- **nDCG@k**: Normalized Discounted Cumulative Gain at k
- **MRR**: Mean Reciprocal Rank
- **Precision@k**: Precision at k results
- **Recall@k**: Recall at k results
- **Search Time**: Query execution time
- **Storage Size**: Collection disk space usage

## Expected Results

Binary embeddings typically provide:
- **32x storage reduction** (4 bytes per float → 1 bit per dimension)
- **2-5x search speedup** (depending on collection size)
- **5-15% quality degradation** in retrieval metrics

The exact trade-offs depend on the dataset, embedding model, and query characteristics.

## Troubleshooting

### Common Issues

**Qdrant connection error:**

```bash
# Check if Qdrant is running
curl http://localhost:6333/health

# Restart Qdrant if needed
docker restart <qdrant-container-id>
```

**Ollama connection error:**

```bash
# Verify Ollama is running and model is available
ollama list
ollama run nomic-embed-text
```

**Out of memory errors:**

- Reduce `--max-entities` parameter when preparing dataset
- Process data in smaller batches

## License

MIT - See [LICENSE](LICENSE) for details

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## Citation

If you use this project in your research, please cite:

```bibtex
@software{binary_embeddings,
  title={Binary Embeddings Comparison},
  author={Victor Lobaco},
  year={2025},
  url={https://github.com/yourusername/binary-embeddings}
}
```
